{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Reto IA: Detección de contenido sensible en tuits de Twitter\n",
    "\n",
    "El trolling es un término del argot de internet que se refiere a una persona que intencionalmente inicia discusiones o molesta a otros publicando comentarios provocadores. El único propósito del trolling es enfadar a las personas. Se ha comparado con el flaming en el contexto del ciberacoso. Además, muchos trolls consideran que lo que hacen es un “arte”. Frecuentemente se esconden detrás del anonimato. El símbolo del trolling es un dibujo en blanco y negro de una cara con una sonrisa traviesa, que simboliza la expresión que alguien hace mientras molesta a sus víctimas.\n",
    "\n",
    "Propósito del trolling:\n",
    "- Ser una fuente de entretenimiento para el troll.\n",
    "- Ser ofensivo y argumentativo.\n",
    "- Obtener placer al molestar intensamente a los demás.\n",
    "- Buscar “presas” en internet (también conocido como tú).\n",
    "- Llamar la atención.\n",
    "- Sentirse poderoso.\n",
    "- Ganar reconocimiento.\n",
    "- Hacer enojar a la víctima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "El Dataset es se conforma por un conjunto de Twits 20001. Cada twits ya posee una etiqueta que asigna una de las dos categorías.\n",
    "#### Métrica de evaluación \n",
    " - Se hará uso de las redes neurales recurrentes (RNN) para clasificar cada twit según la clase a la que corresponda.\n",
    " \n",
    "#### Estrategia de resolución\n",
    "1. Se debe realizar el siguiente pre-procesado del texto de los Twits. Este está compuesto por:\n",
    "    - Limpieza\n",
    "    - Tokenizer\n",
    "    - Pad_sequences (Acá es necesario analizar la base de datos para seleccionar un número óptimo de secuencias)\n",
    "\n",
    "\n",
    "2. Se deben evaluar diferentes arquitecturas de red para evaluar la precisión del modelo para la clasificación. Estas arquitecturas inicialmente están compuestas por las capas: Secuantial(),Embedding(), LSTM(), Dropout(), Dense(). Los parámetros a evaluar son la dimensión de la capa Embedding() para los valores [32,64,128] y las de la capa LSTM() para los valores [64,128,196]. La función de pérdida será una suma cuadrática. Al final se obtendrá un vector de probabilidades, por lo que se puede asignar la categoría en función de la probabilidad.\n",
    "    \n",
    "3. Evaluar algunas variaciones en el modelo (Opcional)\n",
    "    - Se cambiará la función de activación de la capa Dense por una Relu \n",
    "    - Se definirá una tasa de aprendizaje de 1e-5 y un decaimiento de 1e-5\n",
    "    - La capa Embedding tendrà una capa de salida 128 elementos\n",
    "    - La capa LSTM tendrá como dimensión del espacio de salida un valor de 196\n",
    "    - La función de perdida será una crosentropía binária\n",
    "    - El modelo tendrá en cuenta el desbalance que hay en la cantidad de datos que hay en las cateogrías.\n",
    "    - El número de epocas será aumentado a 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('Dataset for Detection of Cyber-Trolls.json',lines = True)\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm \n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,LSTM,Dropout,GRU,SimpleRNN,Embedding\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
